{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import zipfile\n",
    " \n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_to_caption = collections.defaultdict(list)\n",
    "mode = \"total\"\n",
    "anno_type = \"caption\"\n",
    "top_k = 5000\n",
    "caption_vector = []\n",
    "img_name_vector = []\n",
    "\n",
    "for key, val in category.items():\n",
    "    for v in val:\n",
    "        json_dir = os.path.join(\"./data/DGU-AI-LAB-Korean-Tourist-Spot-Dataset-62a483a/kts//\" + mode, key, v, v + \".json\")\n",
    "        image_folder_dir = os.path.join(\n",
    "            \"./data/DGU-AI-LAB-Korean-Tourist-Spot-Dataset-62a483a/kts//\" + mode, key, v, \"images\")\n",
    "\n",
    "        with open(json_dir, encoding=\"UTF-16\") as f:\n",
    "            js = json.load(f)\n",
    "        f.close()\n",
    "\n",
    "        for j in js:\n",
    "            image_file_dir = os.path.join(\n",
    "                image_folder_dir, j[\"img_name\"] + \".jpg\")\n",
    "            # image = Image.open(image_file_dir)\n",
    "            # j[\"image\"]   = image\n",
    "            if anno_type==\"caption\":\n",
    "                image_path_to_caption[image_file_dir] = f\"<start> {j['text']} <end>\"\n",
    "            else:\n",
    "                image_path_to_caption[image_file_dir] = f\"<start> {j['hashtag']} <end>\"\n",
    "\n",
    "        image_paths = list(image_path_to_caption.keys())\n",
    "                \n",
    "        random.shuffle(image_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total Set :  4000\n"
     ]
    }
   ],
   "source": [
    "for image_path in image_paths:\n",
    "    caption = image_path_to_caption[image_path]\n",
    "    caption_vector.append(caption)\n",
    "    img_name_vector.append(image_path)\n",
    "\n",
    "print(\"total Set : \", len(caption_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get unique images\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "# Feel free to change batch_size according to your system configuration\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "./data/DGU-AI-LAB-Korean-Tourist-Spot-Dataset-62a483a/kts//total\\nature-scene\\beach\\images\\1.jpg\n"
     ]
    }
   ],
   "source": [
    "import numpy \n",
    "\n",
    "for item in image_dataset:\n",
    "    print(item.numpy().decode('utf-8'))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'map'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-b1808016a428>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m image_dataset = encode_train.map(\n\u001b[0m\u001b[0;32m      2\u001b[0m     load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "image_dataset = encode_train.map(\n",
    "    load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, path in image_dataset:\n",
    "    batch_features = image_features_extract_model(img)\n",
    "    batch_features = tf.reshape(batch_features,\n",
    "        (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    "    for bf, p in zip(batch_features, path):\n",
    "        print(p)\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        np.save(path_of_feature, bf.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose the top 5000 words from the vocabulary\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                oov_token=\"<unk>\",\n",
    "                                                filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(caption_list)\n",
    "train_seqs = tokenizer.texts_to_sequences(caption_list)\n",
    "\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "# Pad each vector to the max_length of the captions\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "\n",
    "\n",
    "# Calculates the max_length, which is used to store the attention weights\n",
    "max_length = calc_max_length(train_seqs)\n",
    "\n",
    "\n",
    "img_to_cap_vector = collections.defaultdict(list)\n",
    "for img, cap in zip(img_name_vector, cap_vector):\n",
    "    img_to_cap_vector[img].append(cap)\n",
    "\n",
    "# Create training and validation sets using an 80-20 split randomly.\n",
    "img_keys = list(img_to_cap_vector.keys())\n",
    "random.shuffle(img_keys)\n",
    "\n",
    "slice_index = int(len(img_keys)*0.8)\n",
    "img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n",
    "\n",
    "img_name_train = []\n",
    "cap_train = []\n",
    "for imgt in img_name_train_keys:\n",
    "    capt_len = len(img_to_cap_vector[imgt])\n",
    "    img_name_train.extend([imgt] * capt_len)\n",
    "    cap_train.extend(img_to_cap_vector[imgt])\n",
    "\n",
    "print(\"train set : \", len(cap_train))\n",
    "\n",
    "img_name_val = []\n",
    "cap_val = []\n",
    "for imgv in img_name_val_keys:\n",
    "    capv_len = len(img_to_cap_vector[imgv])\n",
    "    img_name_val.extend([imgv] * capv_len)\n",
    "    cap_val.extend(img_to_cap_vector[imgv])\n",
    "\n",
    "print(\"validation set : \", len(cap_val))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "        map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "# dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "# dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "print(\"data set load complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "zip file is ready but not uncompressed.\n",
      "start uncompress..\n",
      "uncompress success\n"
     ]
    }
   ],
   "source": [
    "encoder = CNN_Encoder(config.embedding_dim)\n",
    "decoder = RNN_Decoder(config.embedding_dim, config.units, config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer()"
   ]
  }
 ]
}